{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a916e5-b7ad-4e92-a118-a189b278e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import optuna\n",
    "\n",
    "from giskard import Dataset, Model, scan, testing\n",
    "import pickle\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "import mlflow \n",
    "import mlflow.catboost\n",
    "import mlflow.sklearn\n",
    "import mlflow.data\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af346c89-40de-4503-90ee-8563280f7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SPARK_COMPAT_VERSION = os.getenv('SPARK_COMPAT_VERSION')\n",
    "SCALA_COMPAT_VERSION = os.getenv('SCALA_COMPAT_VERSION')\n",
    "CATBOOST_SPARK_VERSION = os.getenv('CATBOOST_SPARK_VERSION')\n",
    "CLICKHOUSE_HOST = os.getenv('CLICKHOUSE_HOST')\n",
    "CLICKHOUSE_PORT = os.getenv('CLICKHOUSE_PORT')\n",
    "CLICKHOUSE_USER = os.getenv('CLICKHOUSE_USER')\n",
    "CLICKHOUSE_PASSWORD = os.getenv('CLICKHOUSE_PASSWORD')\n",
    "ACCESS_KEY=os.getenv('MINIO_ACCESS_KEY')\n",
    "SECRET_KEY=os.getenv('MINIO_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4611343e-78af-4c09-875b-a4d3ba88e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_TYPES = {\n",
    "    'age': 'category',\n",
    "    'sex': 'category',\n",
    "    'job': 'category',\n",
    "    'housing': 'category',\n",
    "    'credit_amount': 'numeric',\n",
    "    'duration': 'numeric'\n",
    "}\n",
    "\n",
    "TARGET_COLUMN_NAME = 'default'\n",
    "FEATURE_COLUMNS = [i for i in COLUMN_TYPES.keys()]\n",
    "FEATURE_TYPES = {i: COLUMN_TYPES[i] for i in COLUMN_TYPES if i != TARGET_COLUMN_NAME}\n",
    "\n",
    "COLUMNS_TO_SCALE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"numeric\"]\n",
    "COLUMNS_TO_ENCODE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99036d0f-8e52-48a9-9eab-e48bb79b52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = clickhouse_connect.get_client(host = 'localhost', \n",
    "                                       port = CLICKHOUSE_PORT, \n",
    "                                       user = CLICKHOUSE_USER, \n",
    "                                       password = CLICKHOUSE_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c868c18-ebe3-4634-b4b2-dc5c20180f28",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3927e3ea-9338-4639-aa63-536ea9478789",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = {\n",
    "    0: 'unskilled and non-resident', \n",
    "    1: 'unskilled and resident', \n",
    "    2: 'skilled', \n",
    "    3: 'highly skilled'\n",
    "}\n",
    "\n",
    "query = fr'''\n",
    "select {reduce(lambda a,b: a + ', ' + b, FEATURE_COLUMNS)}\n",
    "from credit.credit\n",
    "'''\n",
    "X = (\n",
    "    pd\n",
    "    .DataFrame(client.query(query).named_results())\n",
    "    .assign(job = lambda x: x['job'].apply(lambda x: job_list[x]))\n",
    ")\n",
    "\n",
    "query = fr'''\n",
    "select {TARGET_COLUMN_NAME}\n",
    "from credit.credit\n",
    "'''\n",
    "y = pd.DataFrame(client.query(query).named_results())\n",
    "\n",
    "df = (\n",
    "    X\n",
    "    .join(y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7960d-da96-49e6-b887-150d2853f93a",
   "metadata": {},
   "source": [
    "# MLflow connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "482cbed9-fa6f-4c46-ac4e-cfdbaa83f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI http://localhost:5000/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 23:25:52 INFO mlflow.tracking.fluent: Experiment with name 'credit_prediction_01.05.2025' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/1', creation_time=1747081552636, experiment_id='1', last_update_time=1747081552636, lifecycle_stage='active', name='credit_prediction_01.05.2025', tags={}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlflow.set_tracking_uri('http://localhost:5000/')\n",
    "print(\"URI\", mlflow.get_tracking_uri())\n",
    "mlflow.set_experiment(f'credit_prediction_{datetime.now().strftime('01.%m.%Y')}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3a6cb-a923-4870-8820-3270581aba12",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2f34df-25df-4117-856f-2a9753e202fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, COLUMNS_TO_SCALE),\n",
    "        (\"cat\", categorical_transformer, COLUMNS_TO_ENCODE)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3279f01f-7a24-46f0-ad37-bf59f305329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preproccessed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef5dc8-ce11-4059-b53e-324179d183d3",
   "metadata": {},
   "source": [
    "### Optimize hyperparams (optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea44e9-bd2a-403b-a53b-3391b3dcab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):    \n",
    "    # CatBoostClassifier hyperparams\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"used_ram_limit\": \"3gb\",\n",
    "    }\n",
    "\n",
    "    # model\n",
    "    estimator = CatBoostClassifier(**param, verbose=False)\n",
    "    \n",
    "    # pipelines\n",
    "    clf_pipeline = Pipeline(steps = [\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", estimator)\n",
    "])\n",
    "    # cross-validation accuracy counting\n",
    "    accuracy = cross_val_score(clf_pipeline, df[FEATURE_COLUMNS], df[TARGET_COLUMN_NAME], cv=10, scoring= 'accuracy').mean()\n",
    "    return accuracy\n",
    "\n",
    "#study = optuna.create_study(direction=\"maximize\", study_name=\"CBC-2023-01-14-14-30\", storage='sqlite:///db/CBC-2023-01-14-14-30.db')\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"credit_classifier\")\n",
    "\n",
    "# Hyperparams searching\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# best result is\n",
    "params = study.best_params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba97f7-8bdd-48b3-b45a-31fc1e1ccea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboostclassifier = Pipeline(steps = [\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", CatBoostClassifier(**params, verbose=False))\n",
    "])\n",
    "catboostclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe02a42-0aeb-4bdf-993c-eb9ffb6d7539",
   "metadata": {},
   "source": [
    "## Wrap dataset with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1565001-06d6-460d-b5e2-7e114bf2e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat([X_test, y_test], axis = 1)\n",
    "giskard_dataset = Dataset(\n",
    "    df = raw_data,\n",
    "    target=TARGET_COLUMN_NAME,\n",
    "    name = \"German credit scoring dataset\",\n",
    "    cat_columns=COLUMNS_TO_ENCODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7daf2c4-33de-4df9-bbdb-111d1725bc65",
   "metadata": {},
   "source": [
    "## Wrap model with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fb196-bd6e-4385-bc32-49165cb2cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "giskard_model = Model(\n",
    "    model=catboostclassifier,\n",
    "    model_type=\"classification\",     # Either regression, classification or text_generation.\n",
    "    name=\"Chunk classification\",\n",
    "    classification_labels=catboostclassifier.classes_,  # Their order MUST be identical to the prediction_function's output order\n",
    "    feature_names=FEATURE_COLUMNS     # Default: all columns of your dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36044d8-cbe6-437e-9a9d-9d28e1f6aa3c",
   "metadata": {},
   "source": [
    "## Scan model with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c231a9-5af7-4a1a-9890-efe3e96e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scan(giskard_model, giskard_dataset, verbose=False)\n",
    "results.to_html(\"giskard_scan_result.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7b143-5840-46e3-acb4-96baea5b1f40",
   "metadata": {},
   "source": [
    "# Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52214d5-82e8-4c73-812a-2c373f72c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket credit-model already exists\n",
      "model.pkl successfully uploaded as object model.pkl to bucket credit-model\n"
     ]
    }
   ],
   "source": [
    "def s3_upload_model():\n",
    "    # Create a client with the MinIO server playground, its access key\n",
    "    # and secret key.\n",
    "    client = Minio(\"s3:9099\",\n",
    "        access_key=ACCESS_KEY,\n",
    "        secret_key=SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    # The file to upload, change this path if needed\n",
    "    source_file = \"model.pkl\"\n",
    "\n",
    "    # The destination bucket and filename on the MinIO server\n",
    "    bucket_name = \"credit-model\"\n",
    "    destination_file = \"model.pkl\"\n",
    "\n",
    "    # Make the bucket if it doesn't exist.\n",
    "    found = client.bucket_exists(bucket_name)\n",
    "    if not found:\n",
    "        client.make_bucket(bucket_name)\n",
    "        print(\"Created bucket\", bucket_name)\n",
    "    else:\n",
    "        print(\"Bucket\", bucket_name, \"already exists\")\n",
    "\n",
    "    # Upload the file, renaming it in the process\n",
    "    client.fput_object(\n",
    "        bucket_name, destination_file, source_file,\n",
    "    )\n",
    "    print(\n",
    "        source_file, \"successfully uploaded as object\",\n",
    "        destination_file, \"to bucket\", bucket_name,\n",
    "    )\n",
    "\n",
    "try:\n",
    "    s3_upload_model()\n",
    "except S3Error as exc:\n",
    "    print(\"error occurred.\", exc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
